{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ten notebook powinien być niemal identyczny jak FINAL - zawiera za to dodatkowe objaśnienia w komentarzach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze stron poniżej pobrałem dane na temat populacji województw i miast oraz gęstości zaludnienia i powierzchni miast\n",
    "\n",
    "https://stat.gov.pl/statystyka-regionalna/rankingi-statystyczne/ludnosc-wedlug-wojewodztw/\n",
    "\n",
    "https://pl.wikipedia.org/wiki/Dane_statystyczne_o_miastach_w_Polsce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper as h\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "price_shift = 50000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_hdf('../input/train_property.h5')\n",
    "test = pd.read_hdf('../input/test_property.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stat_miasta = pd.read_csv('../externalData/statystyki-miasta-wiki.csv')\n",
    "stat_woj = pd.read_csv('../externalData/statystyki-woj.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "#\n",
    "# Funkcje pomocnicze\n",
    "#\n",
    "\n",
    "#\n",
    "# konwersja daty na INT na przykład 12 września 2018 -> 201809 \n",
    "# (pomijam dzień, bo dzięki temu od razu mam kategoryzację na miesiące)\n",
    "#\n",
    "def norm_date(value):\n",
    "    if value is None: return value\n",
    "    \n",
    "    months_to_digit = {\n",
    "        'stycznia': 1,\n",
    "        'lutego': 2,\n",
    "        'marca': 3,\n",
    "        'kwietnia': 4,\n",
    "        'maja': 5,\n",
    "        'czerwca': 6,\n",
    "        'lipca': 7,\n",
    "        'sierpnia': 8,\n",
    "        'września': 9,\n",
    "        'października': 10,\n",
    "        'listopada': 11,\n",
    "        'grudnia': 12\n",
    "    }\n",
    "    values = value.split(' ')\n",
    "\n",
    "    day   = int(values[0]) if len(values) == 3 else None\n",
    "    month = values[-2].lower()\n",
    "    year  = int(values[-1])\n",
    "\n",
    "    month = months_to_digit[month]\n",
    "\n",
    "    return (year*100 + month)\n",
    "\n",
    "#\n",
    "# mapowanie tego co znalazłem w 'location' - jeśli istnieje miasto o takiej nazwie\n",
    "# to przypisuje, w przeciwnym wypadku - 'unknown'\n",
    "#\n",
    "def map_city(x):\n",
    "    if (len(x) == 2):\n",
    "        if x[1] in miasta_ll_dict:\n",
    "            return x[1]\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    if (len(x)>2):\n",
    "        if x[2] in miasta_ll_dict:\n",
    "            return x[2]\n",
    "        elif x[1] in miasta_ll_dict:\n",
    "            return x[1]\n",
    "        else: \n",
    "            return 'unknown'\n",
    "    \n",
    "    return 'unknown'\n",
    "\n",
    "#\n",
    "# wyciąganie 'visits' z pola 'stats'\n",
    "#\n",
    "def get_visit_ads(x):\n",
    "    if 'visit_ads' in x:\n",
    "        return np.log( int(x['visit_ads']) + 10 )\n",
    "    return -1\n",
    "\n",
    "#\n",
    "# wyciąganie ceny podanej w słownym opisie (w polu text)\n",
    "#\n",
    "def extract_largest_value(x):\n",
    "    formated = x.lower().replace('m2','').replace(' ','')\n",
    "    digits = re.split(\"\\D\", formated)\n",
    "    max_value = 0\n",
    "    if x.lower().find('cena') > 0: # tylko kiedy w tekscie jest 'cena' \n",
    "        for i in digits:\n",
    "            if i.isdigit():\n",
    "                if len(i) < 7: # ten warunek obcina numery telefonów i inne długie liczbowe identyfikatory\n",
    "                    if int(i) < 1000000: # w ten sposób odrzucam liczby za duże żeby mogły być użyteczną ceną\n",
    "                        max_value = max(int(i),max_value)\n",
    "\n",
    "        if max_value > 100000: # w ten sposób odrzucam za małe ceny (np. ceny za m2)\n",
    "            return(max_value)\n",
    "    \n",
    "    return 0\n",
    "\n",
    "##########################################################################\n",
    "#\n",
    "# Funkcje do kategoryzacji - ten zestaw funkcji dba o to żeby w TRAIN i TEST te same rzeczy dostały \n",
    "# tą samą kategorię, przy czym nie muszę łączyć train i test na czas kategoryzacji. Kategorie są \n",
    "# tworzone na podstawie danych z train a test jest wypełniany danymi. Jeśli czegoś nie było w train, \n",
    "# to test w tym miejscu dostaje 'unkonw'. To pozwala kategoryzować także dane które \"przyjdą później\"\n",
    "# bo słowniki kategoryzacji są zapisane w zmiennej. \n",
    "#\n",
    "\n",
    "def categorize_feature(df, feat, feat_cat, indexers, del_feat=True, average_func=np.median, unknown_cat_name='unknown', unknown_cat_val=-1):\n",
    "    if feat in indexers:\n",
    "        return categorize_feature_for_test(df, feat, feat_cat, indexers[feat], del_feat=del_feat, unknown_cat_name=unknown_cat_name, unknown_cat_val=unknown_cat_val)\n",
    "    \n",
    "    categories_map = categorize_feature_for_train(df, feat, feat_cat, indexers, del_feat=del_feat, unknown_cat_name=unknown_cat_name)\n",
    "    \n",
    "def categorize_feature_for_test(df, feat, feat_cat, categories_map, del_feat=True, unknown_cat_name='unknown', unknown_cat_val=-1):\n",
    "    df[feat_cat] = [categories_map[x] if x in categories_map \n",
    "                    else categories_map[unknown_cat_name] if unknown_cat_name in categories_map \n",
    "                    else unknown_cat_val for x in df[feat]]\n",
    "    \n",
    "    if del_feat:\n",
    "        del df[feat]\n",
    "    \n",
    "def categorize_feature_for_train(df, feat, feat_cat, indexers, del_feat=True, unknown_cat_name='unknown'):\n",
    "    df.loc[df[feat].isnull(), feat] = unknown_cat_name\n",
    "\n",
    "    unique_categories = sorted(list(set(df[feat])),key=str)\n",
    "    \n",
    "    categories_map = {}\n",
    "\n",
    "    for i, (cat) in enumerate(unique_categories):\n",
    "        categories_map[cat] = i\n",
    "\n",
    "    for cat, ind in categories_map.items():\n",
    "        df.loc[df[feat] == cat, feat_cat] = ind\n",
    "\n",
    "    indexers[feat] = categories_map\n",
    "    \n",
    "    if del_feat:\n",
    "        del df[feat]\n",
    "\n",
    "##########################################################################\n",
    "#\n",
    "# Przygotowanie danych\n",
    "#\n",
    "\n",
    "def perform_engineering(df, train_indexers=None):\n",
    "    indexers = train_indexers if train_indexers != None else {}\n",
    "    \n",
    "    df['area_float'] = df['area'].map(lambda x: float(re.sub('[^0-9\\,\\.]','', x).replace(',', '.')))\n",
    "    # fix values lower than zero in area feature\n",
    "    df['area_fixed'] = df['area_float'].map(lambda x: x if x > 0 else 60)\n",
    "    df['floor_int'] = df['floor'].map({'parter':0, '1':1, '2':2, '3':3, -1:-1, '4':4, \n",
    "                                       '7':7, '5':5, '10':10, '8':8, '6':6, '9':9,'> 10':11, \n",
    "                                       'poddasze':12, 'suterena':-2})\n",
    "    df['floors_in_int'] = df['floors_in_building'].map(lambda x: \n",
    "                                                       int(re.sub('[^0-9\\,\\.]','', x)) if x != -1 else -1 )\n",
    "    df['rok_budowy_int'] = df['rok budowy'].map(lambda x: int(re.sub('[^0-9\\,\\.]','', x)) if x != -1 else -1 )\n",
    "    df['czynsz_float'] = df['czynsz'].map(lambda x: \n",
    "                                          float(re.sub('[^0-9\\,\\.]','', x).replace(',', '.')) if x != -1 else x )\n",
    "    \n",
    "    #\n",
    "    # LOCATION\n",
    "    #\n",
    "    \n",
    "    # województwa\n",
    "    df['location_v'] = df['location'].map(lambda x: x[0])\n",
    "    # drugi poziom lokalizacji\n",
    "    df['location_c'] = df['location'].map(lambda x: x[0] + \", \" + x[1])\n",
    "    # trzeci poziom lokalizacji\n",
    "    df['location_cc'] = df['location'].map(\n",
    "            lambda x: x[0] + \", \" + x[1] + \", \" + x[2] if len(x)>2 else x[0] + \", \" + x[1])  \n",
    "    # powiaty\n",
    "    df['location_powiat'] = df['location'].map(lambda x: x[1] if x[1][-1]=='i' else 'unknown')\n",
    "    # miasta\n",
    "    df['location_miasto'] = df['location'].map(map_city)\n",
    "    \n",
    "    # wypełnienie nowej cechy używając populacji w mieście które jet przypisane\n",
    "    df['miasto_ludnosc'] = df['location_miasto'].map(miasta_ll_dict)\n",
    "    # wypełnienie nowej cechy używając gęstości zaludnienia w mieście które jet przypisane\n",
    "    df['miasto_gestosc'] = df['location_miasto'].map(miasta_gz_dict)\n",
    "    # wypełnienie nowej cechy używając powierzchni w mieście które jet przypisane\n",
    "    df['miasto_powierzchnia'] = df['location_miasto'].map(miasta_pow_dict)\n",
    "    # wypełnienie nowej cechy używając populacji w województwie które jet przypisane\n",
    "    df['wojewodztwo_ludnosc'] = df['location_v'].map(wojewodztwa_ll)\n",
    "    \n",
    "    #\n",
    "    # tworzenie kategorii dla lokalizacji\n",
    "    #\n",
    "    categorize_feature(df, 'location_v', 'location_v_cat', indexers=indexers)\n",
    "    categorize_feature(df, 'location_c', 'location_c_cat', indexers=indexers)\n",
    "    categorize_feature(df, 'location_cc', 'location_cc_cat', indexers=indexers)\n",
    "\n",
    "    categorize_feature(df, 'location_powiat', 'location_powiat_cat', indexers=indexers)\n",
    "    categorize_feature(df, 'location_miasto', 'location_miasto_cat', indexers=indexers)\n",
    "    \n",
    "    #\n",
    "    # Stats - wyciąganie danych z pola\n",
    "    #\n",
    "    \n",
    "    df['stats_created_at'] = df['stats'].map(lambda x: x['created_at'])\n",
    "    df['stats_updated_at'] = df['stats'].map(lambda x: x['updated_at'])\n",
    "    df['stats_visit_ads'] = df['stats'].map(get_visit_ads)\n",
    "    \n",
    "    # kategoryzacja danych ze stats\n",
    "    categorize_feature(df, 'stats_created_at', 'stats_created_at_cat', indexers=indexers)\n",
    "    categorize_feature(df, 'stats_updated_at', 'stats_updated_at_cat', indexers=indexers)\n",
    "    \n",
    "    #\n",
    "    # pole text - wyciągamy cenę z tekstowego opisu oferty - czasami jest tam \n",
    "    # podana cena za nieruchomość. Staramy się znaleźć takie wartości i ich tutaj użyć\n",
    "    # - patrz funkcja pomocnicza extract_largest_value\n",
    "    #\n",
    "    \n",
    "    df['largest_value'] = df['text'].map(extract_largest_value)\n",
    "    \n",
    "    #\n",
    "    # kategoryzacja pozostałych cech\n",
    "    #\n",
    "    categorize_feature(df, 'materiał budynku', 'material_cat', indexers=indexers)\n",
    "    categorize_feature(df, 'okna', 'okna_cat', indexers=indexers)\n",
    "    categorize_feature(df, 'stan wykończenia', 'stan_wyk_cat', indexers=indexers)\n",
    "    categorize_feature(df, 'rodzaj zabudowy', 'rodzaj_zabudowy_cat', indexers=indexers)\n",
    "    categorize_feature(df, 'ogrzewanie', 'ogrzewanie_cat', indexers=indexers)\n",
    "    categorize_feature(df, 'forma własności', 'forma_wlasnosci_cat', indexers=indexers)\n",
    "    \n",
    "    #\n",
    "    # czyszczenie dataframe\n",
    "    #\n",
    "    columns_to_remove = ['area', 'location', 'data rozpoczęcia', 'stan inwestycji', 'liczba kondygnacji',\n",
    "                         'floor','floors_in_building','rok budowy', 'czynsz', 'dostępne od',\n",
    "                         'garaż', 'tarasy', 'ochrona', 'stats', 'text', 'rolety antywłamaniowe', \n",
    "                         'kuchenka', 'klimatyzacja', 'plan zagospodarowania:',\n",
    "                         'telefon', 'telewizja kablowa', 'pom. użytkowe', 'pralka', 'piekarnik',\n",
    "                         'lodówka', 'ogródek', 'drzwi / okna antywłamaniowe'\n",
    "                        ]\n",
    "    for col_to_remove in columns_to_remove:\n",
    "        if col_to_remove in df: del df[col_to_remove]\n",
    "\n",
    "    return df, indexers\n",
    "\n",
    "#\n",
    "# cechy oparte na cenach są tworzone na podstawie średnich i median cen za metr wyliczonych\n",
    "# dla danego obszaru (lokalizacje poziomu 1, 2 i 3). Później taka średnia cena jest dla \n",
    "# danej nieruchomości mnożona przez powierzchnię nieruchomości i w ten sposób otrzymujemy\n",
    "# sześć propozycji cen do wykorzystania przy predykcji\n",
    "#\n",
    "def price_engineering(df):\n",
    "    df['lv_mean'] = df['location_v_cat'].map(dict_mean_price_by_LV)\n",
    "    df['lv_median'] = df['location_v_cat'].map(dict_median_price_by_LV)\n",
    "    df['lc_mean'] = df['location_c_cat'].map(dict_mean_price_by_LC)\n",
    "    df['lc_median'] = df['location_c_cat'].map(dict_median_price_by_LC)\n",
    "    df['lc_mean'] = df['lc_mean'].fillna(df['lv_mean'])\n",
    "    df['lc_median'] = df['lc_median'].fillna(df['lv_median'])\n",
    "    df['lcc_mean'] = df['location_cc_cat'].map(dict_mean_price_by_LCC)\n",
    "    df['lcc_median'] = df['location_cc_cat'].map(dict_median_price_by_LCC)\n",
    "    df['lcc_mean'] = df['lcc_mean'].fillna(df['lc_mean'])\n",
    "    df['lcc_median'] = df['lcc_median'].fillna(df['lc_median'])\n",
    "\n",
    "    df['lv_mean_price_calculated'] = df['lv_mean'] * df['area_fixed']\n",
    "    df['lv_median_price_calculated'] = df['lv_median'] * df['area_fixed']\n",
    "    df['lc_mean_price_calculated'] = df['lc_mean'] * df['area_fixed']\n",
    "    df['lc_median_price_calculated'] = df['lc_median'] * df['area_fixed']\n",
    "    df['lcc_mean_price_calculated'] = df['lcc_mean'] * df['area_fixed']\n",
    "    df['lcc_median_price_calculated'] = df['lcc_median'] * df['area_fixed']   \n",
    "    \n",
    "    #\n",
    "    # czyszczenie\n",
    "    #\n",
    "    columns_to_remove = ['price_per_meter']\n",
    "    for col_to_remove in columns_to_remove:\n",
    "        if col_to_remove in df: del df[col_to_remove]\n",
    "\n",
    "#########################################\n",
    "#            \n",
    "# statystyki dla miast i województw\n",
    "#\n",
    "miasta_gz_dict = stat_miasta.groupby('Miasto').agg(np.median)['Gęstość zaludnienia'].to_dict()\n",
    "miasta_ll_dict = stat_miasta.groupby('Miasto').agg(np.median)['Liczba ludności'].to_dict()\n",
    "miasta_pow_dict = stat_miasta.groupby('Miasto').agg(np.median)['Powierzchnia'].to_dict()\n",
    "wojewodztwa_ll = stat_woj.groupby('Województwo').agg(np.median)['Ogółem'].to_dict()\n",
    "            \n",
    "## fill NA - niektóre z funkcji powyżej nie lubią pustych danych\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "## prepare data - pierwsza faza przygotowania danych\n",
    "%time train, indexers = perform_engineering(train)\n",
    "%time test, dummy = perform_engineering(test, indexers)\n",
    "\n",
    "#########################################\n",
    "#\n",
    "# tutaj na podstawie danych z train przygotowujemy cenę za metr dla każdego mieszkania \n",
    "# a co za tym idzie cenę średnią i medianę za metr w lokalizacjach 1, 2 i 3 poziomu\n",
    "#\n",
    "train['price_per_meter'] = train['price'] / train['area_fixed']\n",
    "\n",
    "dict_mean_price_by_LV = train.groupby('location_v_cat').agg(np.mean)['price_per_meter'].to_dict()\n",
    "dict_median_price_by_LV = train.groupby('location_v_cat').agg(np.median)['price_per_meter'].to_dict()\n",
    "dict_mean_price_by_LC = train.groupby('location_c_cat').agg(np.mean)['price_per_meter'].to_dict()\n",
    "dict_median_price_by_LC = train.groupby('location_c_cat').agg(np.median)['price_per_meter'].to_dict()\n",
    "dict_mean_price_by_LCC = train.groupby('location_cc_cat').agg(np.mean)['price_per_meter'].to_dict()\n",
    "dict_median_price_by_LCC = train.groupby('location_cc_cat').agg(np.median)['price_per_meter'].to_dict()\n",
    "\n",
    "## tutaj obrabiamy train i test dodając cechy oparte na średnich cenach\n",
    "%time price_engineering(train)\n",
    "%time price_engineering(test)\n",
    "\n",
    "## fill NA - jeszcze raz na wszelki wypadek\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#\n",
    "# Przygotowanie cech\n",
    "#\n",
    "\n",
    "black_list = ['price', 'id']\n",
    "\n",
    "bool_features = train.select_dtypes(include=[np.bool]).columns.values.tolist()\n",
    "\n",
    "cat_feats = [feat for feat in train.columns if 'cat' in feat]\n",
    "cat_feats = cat_feats + ['rooms', 'is_private', 'floor_int', 'floors_in_int']\n",
    "\n",
    "numeric_features = train.select_dtypes(include=[np.float64, np.int64, np.int8]).columns.values\n",
    "numeric_features = [feat for feat in numeric_features if feat not in (black_list + cat_feats) ]\n",
    "\n",
    "feats = bool_features + numeric_features + cat_feats\n",
    "\n",
    "feats = [feat for feat in feats if feat not in (black_list)]\n",
    "\n",
    "#\n",
    "# przygotowanie zestawu treningowego\n",
    "#\n",
    "\n",
    "X = train[ feats ].values\n",
    "y = train[ 'price' ].values\n",
    "\n",
    "print(\"Selected features: \", feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global fair_constant \n",
    "\n",
    "#\n",
    "# to jest funkcja wewnętrznej metryki dla XGB\n",
    "#\n",
    "def fair_obj(y_true, y_pred):\n",
    "    x = y_pred - y_true\n",
    "    \n",
    "    global fair_constant \n",
    "\n",
    "    den = abs(x) + fair_constant\n",
    "    \n",
    "    grad = fair_constant * x / den\n",
    "    hess = (fair_constant * fair_constant) / (den * den)\n",
    "    \n",
    "    return grad, hess\n",
    "\n",
    "#\n",
    "# wartości użyte w tych modelach zostały wyliczone za pomocą hyperopt\n",
    "# i w niektórych wypadkach nieco poprawione ręcznie podczas testów w zwykłej \n",
    "# pętli dla kolejnych wartości które chciałem sprawdzić\n",
    "#\n",
    "xgb_params_1 = {\n",
    "    'objective': fair_obj,\n",
    "    \n",
    "    'n_jobs': 4, \n",
    "    'max_depth': 8, \n",
    "    'n_estimators': 2500, \n",
    "    'learning_rate': 0.04, \n",
    "    'min_child_weight': 8, \n",
    "    'random_state': 4096\n",
    "}\n",
    "\n",
    "xgb_params_2 = {\n",
    "    'objective': fair_obj,\n",
    "    \n",
    "    'n_jobs': 4, \n",
    "    'max_depth': 14, \n",
    "    'n_estimators': 420, \n",
    "    'learning_rate': 0.056693922378212164, \n",
    "    'min_child_weight': 8, \n",
    "    'random_state': 2018\n",
    "}\n",
    "\n",
    "xgb_params_s = {\n",
    "    'objective': fair_obj,\n",
    "    \n",
    "    'n_jobs': 4, \n",
    "    'max_depth': 8, \n",
    "    'n_estimators': 200, \n",
    "    'learning_rate': 0.05, \n",
    "    'min_child_weight': 8, \n",
    "    'random_state': 2018\n",
    "}\n",
    "\n",
    "shift = price_shift\n",
    "fair_constant = 13259.556042072305 # uzyskane używając hyperopt\n",
    "y_log = np.log(y + shift) # użyłem obliczeń w logarytmach bo wyniki są znacząco lepsze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## pierwszy model\n",
    "##\n",
    "\n",
    "modelXGB1 = XGBRegressor(**xgb_params_1)\n",
    "modelXGB1.fit(X, y_log)\n",
    "    \n",
    "print('first training finished')    \n",
    "    \n",
    "##\n",
    "## drugi model\n",
    "##\n",
    "\n",
    "modelXGB2 = XGBRegressor(**xgb_params_2)\n",
    "modelXGB2.fit(X, y_log)\n",
    "\n",
    "print('second training finished')\n",
    "\n",
    "#\n",
    "# przygotowanie danych do 'stackowania' i dodanie ich do zbioru treningowego\n",
    "#\n",
    "train['XGB_1_results'] = modelXGB1.predict(X)\n",
    "train['XGB_2_results'] = modelXGB2.predict(X)\n",
    "\n",
    "stacked_feats = feats + ['XGB_1_results', 'XGB_2_results']\n",
    "\n",
    "#\n",
    "# trenowanie stacku używając wcześniejszych danych i nowych cech z dwóch poprzednich modeli\n",
    "#\n",
    "\n",
    "X_stacked = train[ stacked_feats ].values\n",
    "y_stacked = train[ 'price' ].values\n",
    "\n",
    "y_stacked_log = np.log(y_stacked + shift)\n",
    "\n",
    "modelXGB_s = XGBRegressor(**xgb_params_s)\n",
    "modelXGB_s.fit(X_stacked, y_stacked_log)\n",
    "\n",
    "print('stacked training finished, ready to predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# LOG ANSWERS - save the answers to the file\n",
    "#\n",
    "\n",
    "#\n",
    "# to jest funkcja do modyfikacji cen - bazuje na danych które wyciągnęliśmy z pola text\n",
    "# - jeśli cena przewidziana przez model różni się od tej którą wyciągnęliśm z tekstu\n",
    "# o mniej niż limit, to zamieniamy ją na tą z tekstu. Nie chcemy skoków o więcej niż limit,\n",
    "# bo czasem z tekstu uda się wyciągnąć jakieś bzdury.\n",
    "#\n",
    "def modPrice(x,limit):\n",
    "    if x['largest_value'] == 0:\n",
    "        return x['price']\n",
    "    if (abs(x['price'] - x['largest_value']) < limit):\n",
    "        return x['largest_value']\n",
    "    \n",
    "    return x['price']\n",
    "\n",
    "#\n",
    "# stacking - przygotowujemy predykcje z dwóch pierwszych modeli i ładujemy je do trzeciego\n",
    "# z którego robimy predykcję\n",
    "# \n",
    "test['XGB_1_results'] = modelXGB1.predict(test[ feats ].values)\n",
    "test['XGB_2_results'] = modelXGB2.predict(test[ feats ].values)\n",
    "stacked_feats = feats + ['XGB_1_results', 'XGB_2_results']\n",
    "y_pred_log = modelXGB_s.predict(test[ stacked_feats ].values)\n",
    "y_pred = np.exp( y_pred_log ) - shift\n",
    "print(\"XGB predictions stacked: \",y_pred)\n",
    "test['priceXGB_s'] = y_pred\n",
    "\n",
    "#\n",
    "# modyfikujemy cenę bazując na cenie z opisu, do limitu 225000\n",
    "#\n",
    "test['price'] =  test['priceXGB_s']\n",
    "test['price_s_mod'] = test.apply(lambda x: modPrice(x,225000), axis=1 )\n",
    "test['price'] = test['price_s_mod']\n",
    "test[ ['id', 'price'] ].to_csv('../output/FINAL_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.csv', index=False) \n",
    "\n",
    "print(\"FINISHED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
